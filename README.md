# ComprehendEdit
## About the project(paper included appendix is [here](https://arxiv.org/abs/2412.12821))   
We introduce **ComprehendEdit**, a comprehensive benchmark with enhanced metrics for multimodal knowledge editing. ComprehendEdit incorporates eight diverse tasks derived from multiple datasets, providing a more robust and varied evaluation framework. Two novel evaluation metrics are introduced: **Knowledge Generalization Index (KGI)** and **Knowledge Preservation Index (KPI)**, which assess the impact of knowledge editing on in-domain samples. The variety in question types of existing datasets (generated by Llama-2-7b-chat-hf) and ComprehendEdit are shown in following table:
| Task                 | E-VQA | VLKEB | ComprehendEdit |
| -------------------- | ----- | ------- | -------------- |
| Object Recognition   | 4,854  | 8,089    | 2,962           |
| Object Attributes    | 1,435  | 27      | 2,987           |
| Object Counting      | 1,213  | 0       | 2,009           |
| Object Existence     | 845   | 3       | 1,962           |
| Scene Information    | 45    | 44      | 2,854           |
| Numerical Inference  | 23    | 0       | 846            |
| Spatial Relationship | 16    | 1       | 2,239           |
| Text Recognition     | 8     | 0       | 2,073           |
| Total                | 8,439  | 8,164    | 17,932          |

ComprehendEdit focus on evaluate the edited model on in-domain samples, as shown in following figure:

<img src="https://github.com/yaohui120/ComprehendEdit/blob/main/figs/illustration_1.png" width="60%">

## Getting Started
### Details of Dataset    
The dataset can be downloaded from [this](https://pan.baidu.com/s/1T5wiMCUlil9DHUOgAewaMg?pwd=6phx).

Here are some samples of ComprehendEdit:

<img src="https://github.com/yaohui120/ComprehendEdit/blob/main/figs/illustration_2.png" width="60%">

**Q, G, P, S, C** mean Question, Ground-truth, Prediction, Source, task Category independently.

The dataset is organized as follows:
```
|——ComprehendEdit/   
|  |——GQA/
|  |  |——images/
|  |  |  |——21.jpg
|  |  |  |——...
|  |——MathVista/
|  |  |——images/
|  |——TallyQA/
|  |  |——VG_100K/            
|  |  |——VG_100K_2/
|  |——TextVQA/
|  |  |——train_images/
|  |——VSR/
|  |  |——images/
|  |——val2014/
|——ComprehendEdit_train.json          
|——ComprehendEdit_test.json
|——ComprehendEdit_ori_right.json          
```

The format of each sample in test set is
```
[{
"image": "GQA/images/2405722.jpg",
"question": "What is this bird called?",
"rephrase": "What is the bird's name?", # for Text-Generality
"answer": "parrot",
"source": "GQA",  
"Category": "object recognition",
"pid": 0,
"img_topk": [...],  # pid of the image topk nearest samples in test set
"txt_topk": [...],  # pid of the text topk nearest samples in test set
"img_last_topk": [...], # pid of the image topk farthest samples in test set
"txt_last_topk": [...], # pid of the text topk farthest samples in test set
"ori_rt_img_topk": [...], # pid of the image topk nearest samples in ComprehendEdit_ori_right.json
"ori_rt_txt_topk": [...], # pid of the text topk nearest samples in ComprehendEdit_ori_right.json
"ori_rt_img_last_topk": [...], # pid of the image topk farthest samples in ComprehendEdit_ori_right.json
"ori_rt_txt_last_topk": [...], # pid of the text topk farthest samples in ComprehendEdit_ori_right.json
"locality_prompt": "when does twice upon a time come out", # for Text-Locality
"locality_ground_truth": "...",
"multimodal_locality_image": "...", # for Multimodal-Locality
"multimodal_locality_prompt": "...",
"multimodal_locality_ground_truth": "..."}, ...]
```

The details of ComprehendEdit is shown in following table:
| Task                 | Train | Test | Source |
| -------------------- | ----- | ---- | -------- |
| Object Recognition   | 1,471  | 491  | GQA      |
| Object Attributes    | 2,227  | 735  | GQA      |
| Object Counting      | 2,282  | 705  | GQA      |
| Object Existence     | 1,506  | 503  | TallyQA  |
| Scene Information    | 2,067  | 787  | GQA      |
| Numerical Inference  | 1,709  | 530  | VSR      |
| Spatial Relationship | 1,554  | 519  | TextVQA  |
| Text Recognition     | 634   | 212  | MathVista|
| Total                | 13,450 | 4,482 |          |

The ratio of training data to test data in each task is approximately 3:1, and we also utilize samples from the NQ dataset and OK-VQA dataset to measure text locality (T-L) and multimodal locality (M-L).

This dataset is collected from several benchmarks using BLIP-2 OPT 2.7B and MiniGPT-4 7B. We recommand measuring the changes on top-10 prediction on locality samples before and after editing if you want to run other models on ComprehendEdit. We will update the results in months.

### Import Dataset   
Working in progress...
### Evaluation
Working in progress...

The code is built based on the EasyEdit, thanks for the framework provided by [EasyEdit](https://github.com/zjunlp/EasyEdit)! The samples in ComprehendEdit comes from several datasets: [GQA](https://cs.stanford.edu/people/dorarad/gqa/index.html), [TallyQA](https://github.com/manoja328/TallyQA_dataset), [VSR](https://github.com/cambridgeltl/visual-spatial-reasoning), [TextVQA](https://textvqa.org/), [MathVista](https://github.com/lupantech/MathVista), [OKVQA](https://okvqa.allenai.org/), and [NQ dataset](https://github.com/google-research-datasets/natural-questions). Thanks for these outstanding works!

Please cite our paper if you use ComprehendEdit in your work.
