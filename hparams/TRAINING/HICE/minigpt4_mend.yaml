# Model
alg_name: "MEND"
device: 2
model_parallel: true
gpu_used_id: [2, 3] #[2, 3, 6] # [0, 1, 2], [3, 4, 5, 6, 7]
gpu_split: ['layers.16'] #['layers.2', 'layers.18'] # ['layers.1', 'layers.11', 'layers.21', 'layers.31'] # ['layers.11', 'layers.27'], ['layers.4', 'layers.13', 'layers.22', 'layers.31']
task: VQA # TextVQA, VQA or Cap

name: /home/myh/hugging_cache/Vicuna-7b1
model_name: minigpt4
model_class: Blip2OPT
tokenizer_class: LlamaTokenizer
tokenizer_name: /home/myh/hugging_cache/Vicuna-7b1
inner_params:
- llama_model.model.layers.29.mlp.down_proj.weight
- llama_model.model.layers.29.mlp.up_proj.weight
- llama_model.model.layers.30.mlp.down_proj.weight
- llama_model.model.layers.30.mlp.up_proj.weight
- llama_model.model.layers.31.mlp.down_proj.weight
- llama_model.model.layers.31.mlp.up_proj.weight

# Method
alg: our
lr: 1e-6
edit_lr: 1e-4
lr_lr: 1e-4
seed: 42
cedit: 0.1
iedit: 0.1
cloc: 1.0
cbase: 1.0
dropout: 0.0
train_base: False
no_grad_layers: null
one_sided: False
n_hidden: 1
hidden_dim: null
init: id
norm: True
combine: True
x_only: False
delta_only: False
act: relu
rank: 1920
mlp_class: IDMLP
shared: True
archive: /home/myh/EasyEdit-main/results/models/MEND/minigpt4_VQA # null

# Train
batch_size: 1
model_save_pt: 5000
silent: False
#max_epochs: 1
max_iters: 30000
log_interval: 100
eval_log_interval: 1000
final_eval: True
val_interval: 5000
early_stop_patience: 20000
early_stop_key: "loss/total_edit_val"
eval_only: False
half: False
debug: False
save: False
verbose: True

val_batch_size: 1
accumulate_bs: 2
val_steps: 500 # only for debug
opt: Adam
grad_clip: 100.

# Output
results_dir: ./results

# Multimodal
exact_match: True
qformer_checkpoint: /home/myh/hugging_cache/blip2_pretrained_flant5xxl.pth
qformer_name_or_path: /home/myh/hugging_cache/bert-base-uncased
state_dict_file: /home/myh/hugging_cache/eva_vit_g.pth
pretrained_ckpt: /home/myh/hugging_cache/pretrained_minigpt4_7b.pth

# image
coco_image: /home/myh/Datasets/MMEdit
rephrase_image: /home/myh/Datasets/MMEdit
